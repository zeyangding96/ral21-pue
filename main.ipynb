{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOxKe7719QvyF8KQmoRc1fh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-KF5GPg_J4m2"},"source":["## Mount google drive. Uncomment these 2 lines if you want to train new models and save them in your drive. Saving in Colab is volatile.\r\n","# from google.colab import drive\r\n","# drive.mount('/content/drive')\r\n","\r\n","## Clone the data and the files from github\r\n","%cd '/content'\r\n","GIT_USERNAME = \"zeyangding96\"\r\n","GIT_REPOSITORY = \"ral20-pue\"\r\n","GIT_PATH = \"https://github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\r\n","!rm -rf \"{GIT_REPOSITORY}\"\r\n","!git clone \"{GIT_PATH}\"\r\n","%cd \"{GIT_REPOSITORY}\"\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import os\r\n","import torch\r\n","import torch.optim as optim\r\n","from torch.utils.data import DataLoader\r\n","from myData import *\r\n","from myUtils import *\r\n","from myModel import *\r\n","\r\n","device = check_gpu(); print('Using', device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmrGId-LLPmo","executionInfo":{"status":"ok","timestamp":1610884895944,"user_tz":-480,"elapsed":1230,"user":{"displayName":"Ze Ding","photoUrl":"","userId":"15900759965780012222"}}},"source":["train_dir = 'data/train'; test_dir = 'data/test'\r\n","input_dim = 2; output_dim = 18\r\n","norm_method = 'scale'\r\n","shuffle = True; lr = 7e-3; wd = 1e-4\r\n","epochs = 2; seq_len = 100; stride = seq_len\r\n","criterion = 'nll'; logvar_dim = 18; epistemic = 'gmm'; num_ensemble = 2; num_MC = 50\r\n","core = 'lstm'; dropout = 0.5\r\n","# savepath = '/content/drive/My Drive/???' ## Specify the savepath if you wish to save the trained models to your google drive. \r\n","\r\n","if num_ensemble == 1:\r\n","    bs = [32]; hidden_dim = [128]\r\n","else:\r\n","    np.random.seed(0)\r\n","    bs = np.random.permutation(range(25,36))[:num_ensemble].tolist()\r\n","    hidden_dim = np.random.permutation(range(120,141))[:num_ensemble].tolist()\r\n","    if len(bs) != num_ensemble: bs += np.random.permutation(range(25,36))[:num_ensemble-len(bs)].tolist()\r\n","    if len(hidden_dim) != num_ensemble: hidden_dim += np.random.permutation(range(120,141))[:num_ensemble-len(hidden_dim)].tolist()\r\n","\r\n","X, Y, norm_param = prepare_data(train_dir, input_dim, output_dim, seq_len, stride)\r\n","X_norm, Y_norm = normalize_data(X, Y, norm_param, norm_method)\r\n","data_train = myDataset(X_norm, Y_norm)\r\n","\r\n","X_Test, Y_Test = load_data_to_timseries(test_dir, input_dim, output_dim, 1, 1)\r\n","x_test, y_test = normalize_data(X_Test, Y_Test, norm_param, norm_method)\r\n","\r\n","## Simulated drift for OOD analysis\r\n","# interval = range(0,900); N = len(interval)\r\n","# x_test[300:600,:,0] += -0.003 * np.arange(300)[:,None]\r\n","# x_test[300:600,:,1] += 0.007 * np.arange(300)[:,None]\r\n","# x_test[interval,:,0] = np.clip(x_test[interval,:,0], -1.5, 1.5); x_test[interval,:,1] = np.clip(x_test[interval,:,1], -1.5, 1.5)\r\n","# x_test = x_test[:1000]; y_test = y_test[:1000]\r\n","\r\n","data_test = myDataset(x_test.transpose(1,0,2), y_test.transpose(1,0,2))\r\n","loader_test = DataLoader(data_test)\r\n","target = torch.tensor(y_test.reshape(-1, output_dim), dtype=torch.float, device='cpu')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1nBJ_LiLP8s"},"source":["d = {'rmse': [], 'nll': []}\r\n","for ii in range(1): ## Average across seeds\r\n","    print('Seed: ', ii)\r\n","    set_rng_seed(ii)\r\n","\r\n","    ## Training\r\n","    Mu_test_ensemble = torch.empty(num_ensemble, 1, x_test.shape[0], output_dim)\r\n","    Var_test_ensemble = torch.empty(num_ensemble, 1, x_test.shape[0], logvar_dim)\r\n","    models = []\r\n","    for j in range(num_ensemble):\r\n","        print('Ensemble: ', j)\r\n","        model = myRnnPue(input_dim, hidden_dim[j], output_dim, logvar_dim, core=core, dropout=dropout, device=device)\r\n","        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\r\n","        loader_train = DataLoader(data_train, batch_size=bs[j], shuffle=shuffle)\r\n","        model.init_train(optimizer, loader_train, loader_val=loader_test)\r\n","        model.Train(epochs=epochs, criterion=criterion, check_train=False, check_val=False, verbose=True)\r\n","        models += [model]\r\n","        Mu_test_ensemble[j], Var_test_ensemble[j], _ = model.validate(loader_test, loss=False)\r\n","    # torch.save(models, savepath+'-seed'+str(ii))\r\n","\r\n","    ## Testing/Inference\r\n","    Mu_test = Mu_test_ensemble.mean(dim=0).squeeze()\r\n","    if epistemic == 'gmm': # gaussian mixture model\r\n","        Var_test = ( (Var_test_ensemble + Mu_test_ensemble**2).mean(dim=0) - Mu_test**2 ).squeeze(dim=0)\r\n","    elif epistemic == 'gal' or epistemic == 'kendall': # MC dropout\r\n","        Mu_test_mc = torch.empty(num_MC, 1, x_test.shape[0], output_dim)\r\n","        Var_test_mc = torch.empty(num_MC, 1, x_test.shape[0], logvar_dim)\r\n","        for j in range(num_MC):\r\n","            Mu_test_mc[j], Var_test_mc[j], _ = model.validate(loader_test, loss=False, mcdropout=True)\r\n","        Mu_test = Mu_test_mc.mean(dim=0).squeeze(dim=0)\r\n","        term1 = (Mu_test_mc**2).mean(dim=0).squeeze(dim=0)\r\n","        term3 = Var_test_mc.mean(dim=0).squeeze(dim=0) if epistemic == 'kendall' else 0\r\n","        Var_test = term1 - Mu_test**2 + term3\r\n","    else:\r\n","        Var_test = Mu_test_ensemble.var(dim=0).squeeze(dim=0)\r\n","    rmse = F.mse_loss(Mu_test, target).sqrt().item()\r\n","    nll = model.nll_loss(Mu_test, torch.log(Var_test), target).item()\r\n","    d['rmse'].append(rmse)\r\n","    d['nll'].append(nll)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7h_9ASq1Yu8t"},"source":["## Here are the codes to generate some of the figures in the paper. I have saved my trained models in \"model_saves\" folder and load it here to plot the figures.\r\n","## New models can also be trained to generate the figures, but they will be slightly different from those in the paper because every network trained is different\r\n","## depending on the random seed and the machines.\r\n","\r\n","## Figure 4: Performance vs number of ensemble. CPU takes a long time to run.\r\n","names = ['lstm-15-nll-gmm-seed0','lstm-15-nll-gmm-seed1','lstm-15-nll-gmm-seed2','lstm-15-nll-gmm-seed3','lstm-15-nll-gmm-seed4']\r\n","rmse_plot_mean = []; rmse_plot_std = []\r\n","nll_plot_mean = []; nll_plot_std = []\r\n","for M in range(1,16):\r\n","    print(M)\r\n","    d = {'rmse': [], 'nll': []}\r\n","    for name in names:\r\n","        models = torch.load('model_saves/'+name, map_location=device)\r\n","        Mu_test_ensemble = torch.empty(M, 1, x_test.shape[0], output_dim)\r\n","        Var_test_ensemble = torch.empty(M, 1, x_test.shape[0], output_dim)\r\n","        for j, model in enumerate(models):\r\n","            model.device = device\r\n","            Mu_test_ensemble[j], Var_test_ensemble[j], _ = model.validate(loader_test, loss=False)\r\n","            if j == M-1: break\r\n","        Mu_test = Mu_test_ensemble.mean(dim=0).squeeze()\r\n","        Var_test = ( (Var_test_ensemble + Mu_test_ensemble**2).mean(dim=0) - Mu_test**2 ).squeeze()\r\n","        rmse = F.mse_loss(Mu_test, target).sqrt().item()\r\n","        nll = model.nll_loss(Mu_test, torch.log(Var_test), target).item()\r\n","        d['rmse'].append(rmse)\r\n","        d['nll'].append(nll)\r\n","    rmse_plot_mean.append( np.mean(d['rmse']) )\r\n","    rmse_plot_std.append( np.std(d['rmse']) )\r\n","    nll_plot_mean.append( np.mean(d['nll']) )\r\n","    nll_plot_std.append( np.std(d['nll']) )\r\n","\r\n","## Plot\r\n","plt.style.use('seaborn-darkgrid')\r\n","fig = plt.figure(figsize=(7,3))\r\n","ax1 = fig.add_subplot()\r\n","plot1 = ax1.plot(rmse_plot_mean, marker='o', markeredgecolor='C0', color='C0', label='RMSE')\r\n","ax1.tick_params(axis='y', labelcolor='C0')\r\n","ax1.set_xlabel('Number of models in the ensemble, $M$', fontsize=14)\r\n","ax1.set_xticks(list(range(0,15,1)))\r\n","ax1.set_xticklabels(['','2','','4','','6','','8','','10','','12','','14'])\r\n","ax2 = ax1.twinx()\r\n","plot2 = ax2.plot(nll_plot_mean, marker='^', markeredgecolor='C1', color='C1', label='NLL')\r\n","ax2.tick_params(axis='y', labelcolor='C1')\r\n","plots = plot1+plot2\r\n","labels = [p.get_label() for p in plots]\r\n","ax1.legend(plots, labels, fontsize=13, loc='best')\r\n","ax1.set_yticks(np.round(np.linspace(ax1.get_ybound()[0], ax1.get_ybound()[1], 5), decimals=4))\r\n","ax2.set_yticks(np.round(np.linspace(ax2.get_ybound()[0], ax2.get_ybound()[1], 5), decimals=4))\r\n","ax1.tick_params(axis='both', labelsize=13)\r\n","ax2.tick_params(axis='y', labelsize=13)\r\n","\r\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPLUsdx9dlmi"},"source":["num_ensemble = 10\r\n","Mu_test_ensemble = torch.empty(num_ensemble, 1, x_test.shape[0], output_dim)\r\n","Var_test_ensemble = torch.empty(num_ensemble, 1, x_test.shape[0], logvar_dim)\r\n","for j in range(num_ensemble):\r\n","    model = torch.load('model_saves/lstm-15-nll-gmm-seed0')[j]\r\n","    Mu_test_ensemble[j], Var_test_ensemble[j], _ = model.validate(loader_test, loss=False)\r\n","Mu_test = Mu_test_ensemble.mean(dim=0).squeeze()\r\n","Var_test = ( (Var_test_ensemble + Mu_test_ensemble**2).mean(dim=0) - Mu_test**2 ).squeeze(dim=0)\r\n","Std_test = Var_test.sqrt()\r\n","Mu_denorm, Std_denorm = denormalize_data(Mu_test.numpy(), norm_param, norm_method, Std_test.numpy())\r\n","colorTrue = 'red'; colorHat = 'blue'; colorFill = 'cornflowerblue'; alpha = 0.5\r\n","lower = (Mu_denorm - 2*Std_denorm).squeeze()\r\n","upper = (Mu_denorm + 2*Std_denorm).squeeze()\r\n","N = range(9000,11000)#range(8000,12000)\r\n","\r\n","plt.style.use('seaborn-darkgrid')\r\n","fig = plt.figure(figsize=(10,10))\r\n","gs = fig.add_gridspec(4,1)\r\n","plt.subplots_adjust(hspace=0.05)\r\n","\r\n","ax1 = fig.add_subplot(gs[0, :]); idx = 9; ## X8. idx 2 to 8 correspond to X1 to X7.\r\n","ax1.plot(Y_Test.squeeze()[N, idx], color=colorTrue)\r\n","ax1.plot(Mu_denorm[N, idx], color=colorHat)\r\n","ax1.fill_between(range(len(N)), lower[N, idx], upper[N, idx], alpha=alpha, color=colorFill)\r\n","# ax1.legend(['Actual', 'Predict', r'$\\pm 2\\sigma$'])\r\n","ax1.set_xticks(list(range(0,4500,500)))\r\n","ax1.set_xticklabels(list(range(0,450,50)))\r\n","plt.setp(ax1.get_xticklabels(), visible=False)\r\n","ax1.set_ylabel('$x_8$ (m)', fontsize=17)\r\n","\r\n","ax2 = fig.add_subplot(gs[1, :], sharex=ax1); idx = -1; ## Y8. idx 10-17 correspond to Y1 to Y7.\r\n","ax2.plot(Y_Test.squeeze()[N, idx], color=colorTrue)\r\n","ax2.plot(Mu_denorm[N, idx], color=colorHat)\r\n","ax2.fill_between(range(len(N)), lower[N, idx], upper[N, idx], alpha=alpha, color=colorFill)\r\n","ax2.legend(['Actual', 'Predict', r'$\\pm 2\\sigma$'], fontsize=14)\r\n","plt.setp(ax2.get_xticklabels(), visible=False)\r\n","ax2.set_ylabel('$y_8$ (m)', fontsize=17)\r\n","\r\n","ax3 = fig.add_subplot(gs[2, :], sharex=ax1); idx = 0; # Force mag\r\n","ax3.plot(Y_Test.squeeze()[N, idx], color=colorTrue)\r\n","ax3.plot(Mu_denorm[N, idx], color=colorHat)\r\n","ax3.fill_between(range(len(N)), lower[N, idx], upper[N, idx], alpha=alpha, color=colorFill)\r\n","plt.setp(ax3.get_xticklabels(), visible=False)\r\n","ax3.set_ylabel('Force Mag. (g)', fontsize=16)\r\n","\r\n","ax4 = fig.add_subplot(gs[3, :], sharex=ax1); idx = 1; # Force loc\r\n","ax4.plot(Y_Test.squeeze()[N, idx], color=colorTrue)\r\n","ax4.plot(Mu_denorm[N, idx], color=colorHat)\r\n","ax4.fill_between(range(len(N)), lower[N, idx], upper[N, idx], alpha=alpha, color=colorFill)\r\n","ax4.set_ylabel('Force Loc. (m)', fontsize=17)\r\n","ax4.set_xlabel('Time (s)', fontsize=17)\r\n","\r\n","ax2.set_yticks(np.round(np.linspace(ax2.get_ybound()[0], ax2.get_ybound()[1], 4), decimals=3))\r\n","ax1.tick_params(axis='y', labelsize=16)\r\n","ax2.tick_params(axis='y', labelsize=16)\r\n","ax3.tick_params(axis='y', labelsize=16)\r\n","ax4.tick_params(axis='both', labelsize=16)\r\n","\r\n","fig.align_ylabels()\r\n","plt.show()"],"execution_count":null,"outputs":[]}]}